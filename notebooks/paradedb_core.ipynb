{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sqlalchemy psycopg2 pgvector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy.sql import exists\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy.sql.schema import MetaData\n",
    "from sqlalchemy import Table, Column, BigInteger, func\n",
    "from sqlalchemy.dialects.postgresql import insert as pginsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define connectionstring\n",
    "# pgurl = 'postgresql://username:password@databasehost:port/databasename'\n",
    "dbname = f'test_corealchemy'\n",
    "tablename = 'tensors'\n",
    "dim = 5\n",
    "pgdburl = f'postgresql+psycopg2://root:root@localhost:54322/{dbname}'\n",
    "pgrootdburl = 'postgresql+psycopg2://root:root@localhost:54322/root'\n",
    "dropdbifexists = True\n",
    "droptableifexists = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rootengine = sqlalchemy.create_engine(pgrootdburl, pool_pre_ping=True, pool_recycle=3600, isolation_level='AUTOCOMMIT', echo=False)\n",
    "dataengine = None\n",
    "\n",
    "def init_data_connections():\n",
    "    global dataengine\n",
    "    dataengine = sqlalchemy.create_engine(pgdburl, pool_pre_ping=True, pool_recycle=3600, isolation_level='AUTOCOMMIT', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_obj = MetaData()\n",
    "tensor_table = Table(\n",
    "    tablename, metadata_obj,\n",
    "    Column(\"key\", BigInteger, primary_key=True, autoincrement=False),\n",
    "    Column(\"embedding\", Vector(dim))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_postgres():\n",
    "    superuserengine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:root@localhost:54322/root', pool_pre_ping=True, pool_recycle=3600, isolation_level='AUTOCOMMIT', echo=False)\n",
    "    with superuserengine.connect() as superuserconnection:\n",
    "        superuserconnection.execute(sqlalchemy.text(\"ALTER SYSTEM SET paradedb.pg_search_telemetry TO 'off';\"))\n",
    "    return 0\n",
    "\n",
    "def init_database():\n",
    "    init_postgres()\n",
    "    ret_val = 0\n",
    "    with rootengine.connect() as rootconnection:\n",
    "        # make sure Telemetry is deactivated \n",
    "        if dropdbifexists:\n",
    "            print(f\"Dropping DB '{dbname}' if exists.\")\n",
    "            if dataengine is not None:\n",
    "                dataengine.dispose()\n",
    "            res = rootconnection.execute(sqlalchemy.text(f'DROP DATABASE IF EXISTS \"{dbname}\" WITH (FORCE);'))\n",
    "        # check if DB exists    \n",
    "        res = rootconnection.execute(sqlalchemy.text('SELECT 1 FROM pg_database WHERE datname=:dbname;'), {'dbname': dbname})\n",
    "        if res.rowcount < 1:\n",
    "            print(f\"Database '{dbname}' does not exist and is beeing created.\")\n",
    "            rootconnection.execute(sqlalchemy.text(f'CREATE DATABASE \"{dbname}\";'))\n",
    "            ret_val = 1\n",
    "        else:\n",
    "            print(f\"Database '{dbname}' exists.\")\n",
    "    if dataengine is None:\n",
    "        init_data_connections()\n",
    "    return ret_val\n",
    "\n",
    "def init_tables():\n",
    "    with dataengine.connect() as dataconnection:\n",
    "        if droptableifexists:\n",
    "            print(f\"Dropping table '{tablename}' in database '{dbname}' if existent.\")\n",
    "            dataconnection.execute(sqlalchemy.text('DROP TABLE IF EXISTS \":tablename\" CASCADE;'), {'tablename': tablename})\n",
    "        print(f\"Creating table '{tablename}' in database '{dbname}' if not exists.\")\n",
    "        # create tables\n",
    "        metadata_obj.create_all(bind=dataconnection, tables=[tensor_table], checkfirst=True)\n",
    "    return 0\n",
    "\n",
    "def table_size():\n",
    "    exists\n",
    "    stmt = sqlalchemy.select(func.count('*')).select_from(tensor_table)\n",
    "    with dataengine.connect() as dataconnection:\n",
    "        count: int = dataconnection.execute(stmt).scalar()\n",
    "        return count\n",
    "\n",
    "nrows = 0 if (init_database()<0 or init_tables()<0) else table_size()\n",
    "print(f'Database {dbname} #rows: {nrows}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((int(1e4), 5))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [{ 'key': i, 'embedding': e } for i,e in enumerate(a)]\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert with error if exists (discouraged)\n",
    "# try:\n",
    "#     with dataengine.begin() as dataconnection: # begin transaction\n",
    "#         dataconnection.execute(tensor_table.insert(), items)\n",
    "# except:\n",
    "#     print('Data already added.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert\n",
    "upsert_stmt = pginsert(tensor_table).on_conflict_do_nothing(index_elements=['key'])\n",
    "with dataengine.begin() as dataconnection: # begin transaction\n",
    "    dataconnection.execute(upsert_stmt, items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve\n",
    "stmt = sqlalchemy.select(tensor_table).where(tensor_table.c.key.in_([1, 2, 7, 8, 12, 241231]))\n",
    "\n",
    "with dataengine.connect() as dataconnection:\n",
    "    rows = dataconnection.execute(stmt)\n",
    "    arr = np.array(list(zip(*rows))[0])\n",
    "    tensors = torch.as_tensor(arr, dtype=torch.float32)\n",
    "    print(tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains\n",
    "key_to_check = 1\n",
    "stmt = sqlalchemy.select(exists().where(tensor_table.c.key == key_to_check))\n",
    "with dataengine.begin() as dataconnection:\n",
    "    item_in_collection = dataconnection.execute(stmt).scalar()\n",
    "print(item_in_collection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove\n",
    "key_to_remove = 4\n",
    "stmt = sqlalchemy.delete(tensor_table).where(tensor_table.c.key == key_to_remove)\n",
    "with dataengine.begin() as dataconnection:\n",
    "    res = dataconnection.execute(stmt)\n",
    "    print(res.rowcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove multiple\n",
    "keys_to_remove = [ 4, 8, 12 ]\n",
    "stmt = sqlalchemy.delete(tensor_table).where(tensor_table.c.key.in_(keys_to_remove))\n",
    "with dataengine.begin() as dataconnection:\n",
    "    res = dataconnection.execute(stmt)\n",
    "    print(res.rowcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ping / is_alive / ready / healthy / ...\n",
    "# @see https://docs.sqlalchemy.org/en/20/core/pooling.html#custom-legacy-pessimistic-ping\n",
    "def ping(stmt, connection):\n",
    "    try:\n",
    "        # run a SELECT 1.   use a core select() so that\n",
    "        # the SELECT of a scalar value without a table is\n",
    "        # appropriately formatted for the backend\n",
    "        return connection.scalar(stmt)\n",
    "    except sqlalchemy.exc.DBAPIError as err:\n",
    "        # catch SQLAlchemy's DBAPIError, which is a wrapper\n",
    "        # for the DBAPI's exception.  It includes a .connection_invalidated\n",
    "        # attribute which specifies if this connection is a \"disconnect\"\n",
    "        # condition, which is based on inspection of the original exception\n",
    "        # by the dialect in use.\n",
    "        if err.connection_invalidated:\n",
    "            # run the same SELECT again - the connection will re-validate\n",
    "            # itself and establish a new connection.  The disconnect detection\n",
    "            # here also causes the whole connection pool to be invalidated\n",
    "            # so that all stale connections are discarded.\n",
    "            return connection.scalar(stmt)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "stmt = sqlalchemy.select(1)\n",
    "try:\n",
    "    with dataengine.connect() as dataconnection:\n",
    "        res = ping(stmt, dataconnection)\n",
    "        # res = datasession.scalar(stmt) # res = 1\n",
    "except sqlalchemy.exc.DBAPIError as err:\n",
    "    res = 0\n",
    "print(res>0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootengine.dispose()\n",
    "dataengine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes for full text search with bm25 \n",
    "\n",
    "# SELECT *\n",
    "# FROM my_table\n",
    "# WHERE my_table @@@ '\"my query string\"'\n",
    "\n",
    "# SELECT *\n",
    "# FROM my_table\n",
    "# WHERE my_table @@@ 'description:keyboard^2 OR electronics:::fuzzy_fields=description&distance=2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from pathlib import PurePosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"paradedb://localhost:5432/fn1.7-sample-bert-unmasked/xasasf?dropifexist&dim=5#fa8gmen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsegments = urlparse(url, allow_fragments=True)\n",
    "queryargs = parse_qs(urlsegments.query, keep_blank_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(urlsegments)\n",
    "print(queryargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posixpath = PurePosixPath(unquote(urlsegments.path))\n",
    "print(posixpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posixpath.parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsegments.username is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
